---
title: "Homework #1: Supervised Learning" 
author: "Caleb Neale"
date: "Due: Tue Feb 08 | 10:30 am"
output: R6018::homework
---

**SYS 6018 | Sp 2022 | University of Virginia**

------------------------------------------------------------------------

```{r config, echo=FALSE}
# source(system.file("config/hw_config.R", package="R6018")) # knitr settings
# options(dplyr.summarise.inform = FALSE)  # ignore dplyr message about grouping
```

# Required R packages and Directories

::: {.solution}
```{r packages, message=FALSE, warning=FALSE}
data.dir = 'https://mdporter.github.io/SYS6018/data/' # data directory
library(R6018)     # functions for SYS-6018
library(tidyverse) # functions for data manipulation    
```
:::

# Problem 1: Evaluating a Regression Model 

## a. Create a set of functions to generate data from the following distributions:
\begin{align*}
X &\sim \mathcal{N}(0, 1) \\
Y &= -1 + .5X + .2X^2 + \epsilon \\
\epsilon &\sim \mathcal{N}(0,\, \sigma)
\end{align*}

::: {.solution}
```{r}
X <- function(n){
  return(
    rnorm(n, mean = 0, sd = 1)
  )
}

e <- function(sigma){
  return(
    rnorm(1, 0, sigma)
  )
}

Y <- function(x, sigma){
  return( 
    -1 +0.5*x+0.2*(x^2)+e(sigma)
    )
}
```
:::

## b. Simulate $n=100$ realizations from these distributions using $\sigma=3$. Produce a scatterplot and draw the true regression line $f(x) = E[Y \mid X=x]$. 
- Use `set.seed(611)` prior to generating the data.

::: {.solution}
```{r}
x <- X(100)
output <- Y(x, 3)

plot(x, output)
```

:::

## c. Fit three polynomial regression models using least squares: linear, quadratic, and cubic. Produce another scatterplot, add the fitted lines and true population line $f(x)$  using different colors, and add a legend that maps the line color to a model.
- Note: The true model is quadratic, but we are also fitting linear (less complex) and cubic (more complex) models. 

::: {.solution}
Add solution here
:::

## d. Simulate a *test data* set of 10,000 observations from the same distributions. Use `set.seed(612)` prior to generating the test data.   
- Calculate the estimated mean squared error (MSE) for each model. 
- Are the results as expected? 

::: {.solution}
Add solution here
:::

## e. What is the best achievable MSE? That is, what is the MSE if the true $f(x)$ was used to evaluate the test set? How close does the best method come to achieving the optimum? 

::: {.solution}
Add solution here
:::

## f. The MSE scores obtained in part *d* came from one realization of training data. Here will we explore how much variation there is in the MSE scores by replicating the simulation many times. 

- Re-run parts b. and c. (i.e., generate training data and fit models) 100 times.
- Calculate the MSE for all simulations. 
- Create kernel density or histogram plots of the resulting MSE values for each model. 
- Use `set.seed(613)` prior to running the simulation and do not set the seed in any other places.
- Use the same test data from part d. (This question is only about the variability that comes from the training data). 

::: {.solution}
Add solution here
:::

## g. Show a count of how many times each model was the best. That is, out of the 100 simulations, count how many times each model had the lowest MSE.

::: {.solution}
Add solution here
:::

## h. Repeat the simulation in part *f*, but use $\sigma=2$. Report the number of times each model was best (you do not need to produce any plots). Use the same `set.seed(613)` prior to running the simulation and do not set the seed in any other places.

::: {.solution}
Add solution here
:::


## i. Repeat *h*, but now use $\sigma=4$ and $n=300$. 

::: {.solution}
Add solution here
:::



